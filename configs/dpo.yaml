# === モデル・トークナイザ ===
model_name_or_path: google/gemma-3-1b-it        # モデル名
dtype: bfloat16                                       # fp16 or bf16
use_fast: true
padding_side: right
chat_template: auto
attn_implementation: eager

# === データセット ===
train_data: cyberagent/chatbot-arena-ja-calm2-7b-chat-experimental # 訓練データ
num_proc: 8                                           # 前処理に使うプロセス数
shuffle: true
max_length: 2048                                  # 最大系列長

# === DPO(DPOConfig) ===
output_dir: outputs/dpo-llama-3-2-1b                  # 保存先
per_device_train_batch_size: 1                        # 学習時のバッチサイズ
per_device_eval_batch_size: 1                         # 検証時のバッチサイズ
gradient_accumulation_steps: 8                        # 勾配累積のステップ数
gradient_checkpointing: true                          # 勾配累積チェックポイント
gradient_checkpointing_kwargs:
  use_reentrant: True
learning_rate: 2.0e-4                                 # 学習率
num_train_epochs: 3                                   # エポック数
lr_scheduler_type: cosine_with_min_lr                 # 学習率スケジューラ
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.03
max_grad_norm: 1.0                                    # 勾配クリッピングの閾値
logging_steps: 10                                     # ログの間隔
save_strategy: "no"                                   # 保存タイミング(no, steps, epoch)
eval_strategy: "no"                                   # 評価間隔(no, steps, epoch)
bf16: true
tf32: true
optim: adamw_torch                                    # 最適化関数
seed: 42
beta: 0.1                                             # DPO の beta パラメータ

# === LoRA(peft) ===
lora_r: 8                                                 # LoRAのrank
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

# === 量子化 ===
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
use_bnb_nested_quant: true

# Deepspeed ===
ds_enable: false
ds_config_file: ds_zero2.json

# === その他 ===
# push-to-hub: false
merge_lora_after_training: false                      # trueでLoRAをベースにマージ保存
