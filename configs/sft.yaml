# === モデル・トークナイザ ===
model_name_or_path: google/gemma-3-1b-it        # モデル名
dtype: bfloat16                                       # fp16 or bf16
use_fast: true
padding_side: right
chat_template: auto

# === データセット ===
train_data: izumi-lab/llm-japanese-dataset            # 訓練データ
num_proc: 8                                           # 前処理に使うプロセス数
shuffle: true
max_length: 4096                                  # 最大系列長

# === SFT(SFTConfig) ===
output_dir: outputs/sft-llama-3-2-1b                  # 保存先
per_device_train_batch_size: 1                        # 学習時のバッチサイズ
per_device_eval_batch_size: 1                         # 検証時のバッチサイズgradient_accumulation_steps: 8                        # 勾配累積チェックポイント
learning_rate: 2.0e-5                                 # 学習率
num_train_epochs: 3                                   # エポック数
lr_scheduler_type: cosine_with_min_lr                 # 学習率スケジューラ
lr_scheduler_kwargs:
  min_lr_rate: 0.1
warmup_ratio: 0.03
max_grad_norm: 1.0                                    # 勾配クリッピングの閾値
logging_steps: 10                                     # ログの間隔
save_strategy: "no"                                     # 保存タイミング(no, steps, epoch)
eval_strategy: "no"                                  # 評価間隔(no, steps, epoch)
bf16: true
tf32: true
optim: adamw_torch                                    # 最適化関数
seed: 42

# === LoRA(peft) ===
lora_r: 16                                                 # LoRAのrank
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

# === 量子化 ===
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4

# Deepspeed ===
ds_enable: false
ds_config_file: ds_zero2.json

# === その他 ===
# push-to-hub: false
merge_lora_after_training: false                      # trueでLoRAをベースにマージ保存
